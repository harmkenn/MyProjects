---
title: "Understand GLM"
author: "Ken Harmon"
date: "`r format(Sys.time(), '%Y %B %d')`"
output:
  html_document:  
    keep_md: true
    code_folding: hide
    fig_height: 6
    fig_width: 12
    fig_align: 'center'
editor_options: 
  chunk_output_type: console
---

# {.tabset .tabset-fade}

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r load_libraries, include=FALSE}
# Use this R-Chunk to load all your libraries!
pacman::p_load(tidyverse, ggExtra, lmisc)
theme_set(theme_bw())
```

```{r swd, eval=FALSE, echo=FALSE}
# this is set to not run during the knit process
# this sets the working directory to the file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

```{r}
#NBA_Player_Stats <- as_tibble(NBA_Player_Season_Stats_1950_2019)
#save(NBA_Player_Stats, file = "NBAPlayerStats1950-2019.rda")
load(file = "NBAPlayerStats1950-2019.rda")
```

https://www.barelysignificant.com/post/glm/

```{r}
#remove.packages("rstan")
#if (file.exists(".RData")) file.remove(".RData")
#install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)


install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))
library(devtools)
devtools::install_github("rmcelreath/rethinking")


```

```{r}
Howell1 <- read_csv("Howell1.csv")
d <- Howell1 %>% filter(age >= 18) # Just adults

p <- d %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_point(pch = 21, color = "white", fill = "black", size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", colour = "black") +
  theme_bw(base_size = 12)

ggMarginal(p, type = "histogram")
```

A quick visual exploration of the dataset reveals a positive relationship between height and weight. The above plotted regression line corresponds to the following model, where we assume a normal likelihood:

This model can be fitted easily in R with the following syntax.

```{r}
(mod1 <- lm(height ~ weight, data = d) )
```

The intercept (113.879) represents the predicted height when weight is at 0 (which makes no much sense in our case), whereas the slope (0.905) represents the change in height when weight increases by one unit (i.e., one kilogram).

Prediction is key: predict and fitted
The main advantage of the previous model is that it allows to make predictions for any value of . In R, this is done using the aptly named predict function. For instance, we can ask our model what is the expected height for an individual of weight 43, which is equal to .

```{r}
wght <- 43

d %>%
  ggplot(aes(x = weight, y = height) ) +
  geom_line(aes(y = predict(mod1) ), size = 1) +
  geom_point(size = 2, alpha = 0.2) +
  geom_segment(
    x = wght, xend = wght,
    y = 0, yend = predict(mod1, newdata = data.frame(weight = wght) ),
    linetype = 2, lwd = 0.5
    ) +
  geom_segment(
    x = 0, xend = wght,
    y = predict(mod1, newdata = data.frame(weight = wght) ),
    yend = predict(mod1, newdata = data.frame(weight = wght) ),
    linetype = 2, lwd = 0.5
    ) +
  theme_bw(base_size = 12)

```

Implementing the function predict by hand is quite easy and will allow us to better understand how it works. This function is actually simply retrieving parameters of the fitted model (in our case, the intercept and the slope) to make predictions about the outcome variable, given some values of the predictor(s). In other words, it corresponds to the second line of our model.

```{r}
d <- d %>%
  mutate(
    pred_mod1 = predict(mod1),
    pred_mod1_2 = coef(mod1)[1] + coef(mod1)[2] * weight
    )

head(d)
```

We could also be interested in predicting the height of individuals with other weights than the weights we observed (e.g., weights between 80 and 100 kgs). Below we simulate new data from our model (i.e., we simulate heights) and predictions for this new set of data (i.e., the ).

```{r}
# generating weights from 80 to 100 kgs
data.frame(weight = 80:100) %>%
  # retrieving mod1 predictions
  mutate(pred = predict(mod1, newdata = .) ) %>%
  # simulating data from our model, taking into account sigma
  mutate(sim =  pred + rnorm(length(weight), 0, sd(residuals(mod1) ) ) ) %>%
  # or using sigma(mod1) instead of sd(residuals(mod1) ), as suggested
  # by Tristan Mahr (https://www.tjmahr.com)
  mutate(sim =  pred + rnorm(length(weight), 0, sigma(mod1) ) ) %>%
  # plotting these predictions
  ggplot(aes(x = weight, y = pred) ) +
  geom_line(size = 1) +
  geom_point(aes(x = weight, y = sim), size = 2) +
  geom_segment(
      aes(xend = weight, yend = sim),
      size = 0.5, alpha = 0.5, lineend = "round"
      ) +
  theme_bw(base_size = 12)
```

Where the vertical lines represent deviations from the predicted values. OK, so we’ve seen that the predict function simply uses the linear model to make predictions about the .

You might know that there exists a similar function, the fitted function, which allows to extract fitted values of a model.

```{r}
d <- d %>% mutate(fitted_mod1 = fitted(mod1) )
head(d)
```

Surprisingly, the predict and fitted functions seem to do the exact same thing (at least their results are the same)… but do they? To answer this question, let’s ask another one.

Link function, toward GLMs
Can we predict gender by individual height? The usual way to answer this kind of question is through a logistic regression model (or logit model). Logistic regression is used to model binary outcome variables, using the linear regression framework. In the logit model, the log-odds of the outcome  are modelled as a linear combination of the predictor variables:

 

Thus, although the observed outcome variable is a dichotomic variable, the logistic regression estimates the log-odds, as a continuous variable, that the outcome variable is in a certain state (in our case, that the individual is a man)1. This model can be written as follows, where  is the probability that an individual is a man.

 

This model is implemented easily in R using the glm function, where the family argument is used to specify the likelihood of the model, and the link function.

```{r}
mod2 <- glm(male ~ height, data = d, family = binomial(link = "logit") )

mod2
```

Below we print predictions of the model, using both the predict and fitted functions.

```{r}
d <- d %>%
  mutate(
    pred_mod2 = predict(mod2),
    fitted_mod2 = fitted(mod2)
    )

d %>%
  select(height, weight, male, pred_mod2, fitted_mod2) %>%
  head
```

This time the results of predict and fitted appear to be quite different… We can plot the predictions of mod2 following the same strategy as previously using fitted. The logit_dotplot function displays the prediction of the logit model along with the marginal distribution of height by gender (detailed code can be found here ).

```{r}
if (!require("devtools") ) install.packages("devtools")
devtools::install_github("lnalborczyk/lmisc", dependencies = TRUE)
#source("logit_dotplot.R")
logit_dotplot(d$height, d$male, xlab = "height", ylab = "p(male)")
```

The output of the predict and fitted functions are different when we use a GLM because the predict function returns predictions of the model on the scale of the linear predictor (here in the log-odds scale), whereas the fitted function returns predictions on the scale of the response. To obtain the fitted values, we thus have to apply the inverse of the link function to the predicted values obtained with predict. In our case, this translates to the logistic transformation:


Which, in R, gives:

```{r}
exp(coef(mod2) ) / (1 + exp(coef(mod2) ) )
```

Which is equivalent to using the plogis function:

```{r}
plogis(coef(mod2) )
```

Let’s compare our previous calls to the predict and fitted functions…

```{r}
d <- d %>%
  mutate(pred_mod2_2 = plogis(predict(mod2) %>% as.numeric) )

d %>%
  select(height, weight, male, pred_mod2, fitted_mod2, pred_mod2_2) %>%
  head
```

To sum up, the fitted function automatically applies the inverse transformation to provide prediction on the scales of the outcome. A similar behaviour can be obtained by using the predict function, and by specifying the scale in which we want to obtain predictions (e.g., in the scale of the response variable).

```{r}
d <- d %>%
  mutate(pred_mod2_3 = predict(mod2, type = "response") )

d %>%
  select(height, weight, male, pred_mod2, fitted_mod2, pred_mod2_2, pred_mod2_3) %>%
  head
```

To relate this to our understanding of the linear model equation, prediction pertains to the , meaning that we try to predict the mean observed outcome for a specific value of the predictor .

We should go further and implement uncertainty in these predictions, but we should first take a break to examine the concepts of errors and residuals, and their relationship.

Errors and residuals: the residuals function
A very common fallacy about the assumptions of the linear (Gaussian) model is that the outcome variable should be normally distributed. Instead, this assumption concerns the distribution of the outcome variable around its predicted value (i.e., the distribution of the errors).

What we actually said above is that the errors  should be normally distributed around the predicted value. But the errors are the the non-observed (and non-observable) differences between the theoretical predicted value  and the observed outcomes. Consequently, we do not have access to it. Instead, what we can work with are the residuals , which can be seen as an estimate (from the sample) of the errors , in a similar way as  is an estimate of . To sum up, the residuals are the 
 whereas the errors are the .

In other words, errors pertain to the data generation process, whereas residuals are the difference between the model’s estimation and the observed outcomes. Basically, the residuals are the difference between the observed value and the predicted value. We can obtain them easily using the residuals function (which is useful for more complex models), or by subtracting to each observed outcome  the predicted .

```{r}
d <- d %>%
  mutate(
    res1 = residuals(mod1),
    res2 = height - pred_mod1
    )

d %>%
  select(height, weight, male, pred_mod1, res1, res2) %>%
  head
```

Below we plot these residuals, and make the alpha (i.e., the transparency) and the size of the points dependent on the distance to the predicted value (so that larger residuals appear as bigger and less transparent). This distance is also represented by the length of the vertical lines.

```{r}
d %>%
  sample_frac(0.5) %>% # selecting a (50%) subsample of the data
  ggplot(aes(x = weight, y = height) ) +
  geom_line(aes(y = pred_mod1), size = 1) +
  geom_point(aes(alpha = abs(res1), size = abs(res1) ) ) +
  guides(alpha = FALSE, size = FALSE) +
  geom_segment(aes(xend = weight, yend = pred_mod1, alpha = abs(res1) ) ) +
  theme_bw(base_size = 12)
```

If we take all the above verticals bars (i.e., the residuals) and plot their distribution, we can compare them to a normal distribution with mean 0, and standard deviation  equal to the standard deviation of the residuals, to check our assumption about the distribution of the residuals (here a normality assumption).

```{r}
d %>%
  ggplot(aes(x = res1) ) +
  geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.6) +
  geom_line(aes(y = dnorm(res1, mean = 0, sd = sd(res1) ) ), size = 1) +
  guides(fill = FALSE) +
  theme_bw(base_size = 12) +
  labs(x = "Residuals", y = "Density")
```


