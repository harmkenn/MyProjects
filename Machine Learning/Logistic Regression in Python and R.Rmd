---
title: "Python and R Logistic Regression"
author: "Ken Harmon"
date: "`r format(Sys.time(), '%Y %B %d')`"
output:
  html_document:  
    keep_md: true
    code_folding: hide
    fig_height: 6
    fig_width: 12
    fig_align: 'center'
editor_options: 
  chunk_output_type: console
---

# {.tabset .tabset-fade}

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r load_libraries, include=FALSE}
# Use this R-Chunk to load all your libraries!
pacman::p_load(tidyverse, reticulate, caTools)
theme_set(theme_bw())
confusionMatrix <- caret::confusionMatrix
select <- dplyr::select
```

```{r swd, eval=FALSE, echo=FALSE}
# this is set to not run during the knit process
# this sets the working directory to the file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

```{r, echo=FALSE}
library(reticulate)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::knit_engines$set(python = reticulate::eng_python)
reticulate::repl_python()
```

##Example of Logistic Regression in Python
Now let us take a case study in Python. We will be taking data from social network ads which tell us whether a person will purchase the ad or not based on the features such as age and salary.

First, we will import all the libraries:

```{python}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

Now we will import the dataset and select only age and salary as the features

```{python}
dataset = pd.read_csv('Social_Network_Ads.csv')
dataset.head()
```

```{python}
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values
```

Now we will perform splitting for training and testing. We will take 75% of the data for training, and test on the remaining data

```{python}
from sklearn.model_selection  import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
```

Next, we scale the features to avoid variation and let the features follow a normal distribution

```{python}
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
```

The preprocessing part is over. It is time to fit the model

```{python}
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)
```

We fitted the model on training data. We will predict the labels of test data.

```{python}
y_pred = classifier.predict(X_test)
```

The prediction is over. Now we will evaluate the performance of our model.

```{python}
from sklearn.metrics import confusion_matrix,classification_report
cm = confusion_matrix(y_test, y_pred)
cl_report=classification_report(y_test,y_pred)
```

Now we can see that our model performed really good based on precision and recall.

##Example of Logistic Regression in R 
We will perform the application in R and look into the performance as compared to Python

First, we will import the dataset

```{r}
dataset = read.csv('Social_Network_Ads.csv')
#We will select only Age and Salary 
dataset = dataset [3:5]
```

Now we will encode the target variable as a factor

```{r}
dataset$Purchased = factor (dataset$Purchased, levels = c (0, 1))
```

Next, we will split the data into training and testing.

```{r}
# install.packages('caTools')
library(caTools)
set.seed(43)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset (dataset, split == TRUE)
test_set = subset (dataset, split == FALSE)
```

Then we will perform scaling only on the feature variables

```{r}
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
```

After preprocessing is done, we will fit the model and get ready for prediction

```{r}
# Fitting Logistic Regression to the Training set
classifier = glm(formula = Purchased ~ .,
                 family = binomial,
                 data = training_set)
```

The fitting is done. Now we predict the values by keeping the threshold as 0.5. It means that the probability above 0.5 is counted as 1 and the rest as 0.

```{r}
prob_pred = predict (classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse (prob_pred > 0.5, 1, 0)
```

Now we will evaluate our model based on the confusion matrix and make a comparison with Python.

```{r}
cm = table (test_set[, 3], y_pred > 0.5)
```

As we see the rate of misclassification is much higher in R than in Python. But we can only challenge it when the data splitting will be the same in both cases.







