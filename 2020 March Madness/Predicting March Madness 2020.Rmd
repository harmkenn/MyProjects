---
title: "Predicting March Madness 2020"
author: "Ken Harmon"
date: "`r format(Sys.time(), '%Y %B %d')`"
output:
  html_document:  
    keep_md: true
    code_folding: hide
    fig_height: 6
    fig_width: 12
    fig_align: 'center'
editor_options: 
  chunk_output_type: console
---

# {.tabset .tabset-fade}

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r load_libraries, include=FALSE}
# Use this R-Chunk to load all your libraries!
pacman::p_load(tidyverse, DT, rpart, DMwR, caret, rpart.plot, rattle, Metrics)
theme_set(theme_bw())
confusionMatrix <- caret::confusionMatrix
```

```{r swd, eval=FALSE, echo=FALSE}
# this is set to not run during the knit process
# this sets the working directory to the file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

## NCAARock

NCAA Season Stats: https://www.sports-reference.com/cbb/seasons/

All Tournament Game Results:

  From which I can get Seed Expectations
  From Which I can get each teams PASE
  
Kenpom Rankings: https://kenpom.com/index.php?s=TeamName

### 2002 to 2019 Games

```{r DT}
# Load NCAARock of all games and the differences of season stats
NCAARock <- read.csv( "NCAARock.csv", stringsAsFactor = FALSE ) 

NCAARock <- NCAARock %>% mutate(result = Fscore - Uscore)

#If you want to see where the NAs are
#sapply(NCAARock, function(x) sum(is.na(x)))
# pull out just the Numeric columns
temp <- NCAARock %>% select(10:42)

#Fill in all of the NAs with the K nearest neighbor
temp <- knnImputation(temp, k = 10, scale = T, meth = "weighAvg", distData = NULL)

#pull of the front columns not in temp
Front <- NCAARock %>% select(1:9)

NCAARock <- cbind(Front,temp)

#binomial the difference in Score
bresult <- ifelse(NCAARock$result > 0, "F", "U")
NCAARock$bresult <- as.factor(bresult)

datatable(NCAARock, extensions = "Responsive",options=list(lengthMenu = c(10,25,68)))
```

## Train and Test

```{r tt}
# Get the number of observations
n_obs <- nrow(NCAARock)

# Shuffle row indices: permuted_rows
permuted_rows <- sample(n_obs)

# Randomly order data: Sonar
NCAA_shuffled <- NCAARock[permuted_rows, ]

# Identify row to split on: split
split <- round(n_obs * 0.6)

# Create train
train <- NCAA_shuffled[1:split, ]

# Create test
test <- NCAA_shuffled[(split + 1):n_obs, ]

nrow(train)/nrow(NCAARock)
```

## Logistic Regression

```{r lr}
# Fit glm model: model
model <- glm(bresult ~ ., family = "binomial", train %>% select(10:41,43))

# Predict on test: p
p <- predict(model, test, type = "response")
```

## Confusion Matrix

```{r ccm}
# If p exceeds threshold of 0.5, M else R: m_or_r
F_or_U <- ifelse(p < .5, "F", "U")


# Convert to factor: p_class
p_class <- factor(F_or_U, levels = levels(test[["bresult"]]))

# Create confusion matrix
confusionMatrix(p_class, test$bresult)
```

## Tree

Build a classification tree

```{r bct}
# Create the model
pred_model <- rpart(formula = bresult ~ ., data = NCAARock %>% select(1:3,6,10:41,43), method = "class", model=TRUE)

# Display the results
rpart.plot(x = pred_model, yesno = 2, type = 0, extra = 0)
```

Train/test split

```{r tts}

# Total number of rows in the NCAA data frame
n <- nrow(NCAARock)

# Number of rows for the training set (80% of the dataset)
n_train <- round(.8 * n) 

# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices <- sample(1:n, n_train)

# Subset the NCAA data frame to training indices only
NCAA_train <- NCAARock[train_indices, ]  
  
# Exclude the training indices to create the test set
NCAA_test <- NCAARock[-train_indices, ] 
```

Train a classification tree model

```{r tctm}
# Train the model (to predict 'bresult')
NCAA_model <- rpart(formula = bresult ~ ., data = NCAA_train %>% select(1:3,6,10:41,43), method = "class", model = TRUE)

# Look at the model output                      
print(NCAA_model)
```

Compute confusion matrix

```{r ccm2}
# Generate predicted classes using the model object

class_prediction <- predict(object = NCAA_model, newdata = NCAA_test, type = "class")  
                            
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction, reference = NCAA_test$bresult) 
```

Compare models with a different splitting criterion

```{r cmdsc}
# Train a gini-based model
NCAA_model1 <- rpart(formula = bresult ~ ., 
                       data = NCAA_train %>% select(1:3,6,10:41,43), 
                       method = "class",
                       parms = list(split = "gini"))

# Train an information-based model
NCAA_model2 <- rpart(formula = bresult ~ ., 
                       data = NCAA_train %>% select(1:3,6,10:41,43), 
                       method = "class",
                       parms = list(split = "information"))

# Generate predictions on the validation set using the gini model
pred1 <- predict(object = NCAA_model1, 
             newdata = NCAA_test %>% select(1:3,6,10:41,43),
             type = "class")  

# Generate predictions on the validation set using the information model
pred2 <- predict(object = NCAA_model2, 
             newdata = NCAA_test %>% select(1:3,6,10:41,43),
             type = "class")

# Compare classification error
ce(actual = NCAA_test$bresult, 
   predicted = pred1)
ce(actual = NCAA_test$bresult, 
   predicted = pred2) 
```

## Regression Trees

Split the Data

```{r std}

# Set seed and create assignment
set.seed(1)
assignment <- sample(1:3, size = nrow(NCAARock), prob = c(.7,.15,.15), replace = TRUE)

# Create a train, validation and tests from the original data frame 
NCAA_train <- NCAARock[assignment == 1, ]    # subset NCAA to training indices only
NCAA_valid <- NCAARock[assignment == 2, ]  # subset NCAA to validation indices only
NCAA_test <- NCAARock[assignment == 3, ]   # subset NCAA to test indices only
```

Train the Regression Tree Model

```{r trtm}
# Train the model
NCAA_model <- rpart(formula = result ~ ., 
                     data = NCAA_train %>% select(1:3,6,10:41,42),
                     method = "anova")

# Look at the model output                      
print(NCAA_model)

# Plot the tree model
rpart.plot(x = NCAA_model, yesno = 2, type = 0, extra = 0)
```

Evaluate the Regression Tree Model

```{r ertm}
# Generate predictions on a test set
pred <- predict(object = NCAA_model,   # model object 
                newdata = NCAA_test %>% select(1:3,6,10:41,42))  # test dataset

# Compute the RMSE
rmse(actual = NCAA_test$result, 
     predicted = pred)
```

Tuning the Model

```{r ttm}
# Plot the "CP Table"
plotcp(NCAA_model)

# Print the "CP Table"
print(NCAA_model$cptable)

# Retrieve optimal cp value based on cross-validated error
opt_index <- which.min(NCAA_model$cptable[, "xerror"])
cp_opt <- NCAA_model$cptable[opt_index, "CP"]

# Prune the model (to optimized cp value)
NCAA_model_opt <- prune(tree = NCAA_model, cp = cp_opt)
                          
# Plot the optimized model
rpart.plot(x = NCAA_model_opt, yesno = 2, type = 0, extra = 0, roundint = FALSE)
```

Generate a grid of hyperparameter values

```{r gghv}
# Establish a list of possible values for minsplit and maxdepth
minsplit <- seq(1, 4, 1)
maxdepth <- seq(1, 6, 1)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)

# Check out the grid
head(hyper_grid)

# Print the number of grid combinations
nrow(hyper_grid)
```

Generate a grid of models

```{r ggm}
# Number of potential models in the grid
num_models <- nrow(hyper_grid)

# Create an empty list to store models
NCAA_models <- list()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:num_models) {

    # Get minsplit, maxdepth values at row i
    minsplit <- hyper_grid$minsplit[i]
    maxdepth <- hyper_grid$maxdepth[i]

    # Train a model and store in the list
    NCAA_models[[i]] <- rpart(formula = result ~ ., 
                               data = NCAA_train%>% select(1:3,6,10:41,42), 
                               method = "anova",
                               minsplit = minsplit,
                               maxdepth = maxdepth)
}
```

Evaluate the Grid

```{r etg}

# Number of potential models in the grid
num_models <- length(NCAA_models)

# Create an empty vector to store RMSE values
rmse_values <- c()

# Write a loop over the models to compute validation RMSE
for (i in 1:num_models) {

    # Retrieve the i^th model from the list
    model <- NCAA_models[[i]]
    
    # Generate predictions on NCAA_valid 
    pred <- predict(object = model,
                    newdata = NCAA_valid %>% select(1:3,6,10:41,42))
    
    # Compute validation RMSE and add to the 
    rmse_values[i] <- rmse(actual = NCAA_valid$result, 
                           predicted = pred)
}

# Identify the model with smallest validation set RMSE
best_model <- NCAA_models[[which.min(rmse_values)]]

# Print the model paramters of the best model
best_model$control

# Compute test set RMSE on best_model
pred <- predict(object = best_model,
                newdata = NCAA_test %>% select(1:3,6,10:41,42))
rmse(actual = NCAA_test$result, 
     predicted = pred)
```




